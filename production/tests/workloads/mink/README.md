# Mink Integration Test

This integration test workload provides for a workload that takes the
standard `Incubator` example and adds a debug mode and lots of measurement
points.  This was meant to be a solid ending point for anyone learning
about the workloads as it is a fully functional and reportable workload.

## See Also

For a workload that has the bare minimum of infrastructure required to create a workload,
please consult the [Barebones Workload](#template).

For a workload that sets a lot of the requirements and infrastructure up, but not the
XXX, please consult the [Template Workload](#template).

For a workload that has everything setup with a very simple model and some simple
measurements, please consult the [XX Workload](#template).

## Workload Input

### Test Properties File

## Workload Output

### Test Results Directory

Upon completion of the test run, the results of that test run are copied into
the `test-results` directory.
While it is not a complete copy of everything that was used to execute the test,
it is a copy of all relevant input files and all relevant files produced by
that test.  Note that at the beginning of each test run, this directory is
deleted.

Common files to see in this directory are:
- `duration.json`
  - JSON output file containing the duration of the test
  - captured around the `run.sh` script using `date +%s.%N`
- `expected.diff`
  - simple file created by `diff`ing the test's optional `expected_output.json` file with the generated `output.json` file
- `gaia.log`
  - clean version of the log file generated by the rule engine
- `gaia_stats.log`
  - clean version of the log file generated by the rule engine
- `mink.conf`
  - configuration file generated from the `config.json` file
- `output.csv`
  - experimental, for output to spreadsheets
  - various test measurements, mostly various time-based delays, in CSV format
- `output.delay`
 - various test measurements, mostly various time-based delays, in JSON format
- `output.json`
  - output file generated by executing the commands
- `output.txt`
  - log of the results of the `test.sh` script in `-v`/verbose mode
- `return_code.json`
  - simple file with the return code emitted by the test

### Test Summary File

#### Available Measures

For all tests, the following general rows are reported.  More information on these fields are available [in this section](#single-test-json-blob).

- `iter-total`
  - aggregation of the `iteration-measured-duration-sec` file from `summary.json`
- `start`
  - aggregation of the `average-start-transaction-microsec` file from `summary.json`
- `inside`
  - aggregation of the `average-inside-transaction-microsec` file from `summary.json`
- `commit`
  - aggregation of the `average-end-transaction-microsec` file from `summary.json`
- `update-row`
  - aggregation of the `average-update-row-microsec` file from `summary.json`
- `wait`
  - aggregation of the `average-wait-microsec` file from `summary.json`
- `print`
  - aggregation of the `average-print-microsec` file from `summary.json`

- `avg-latency`
  - aggregation of the `rules-engine-stats.calculations.avg-latency-ms` file from `summary.json`
- `max-latency`
  - aggregation of the `rules-engine-stats.calculations.max-latency-ms` file from `summary.json`
- `avg-exec`
  - aggregation of the `rules-engine-stats.calculations.avg-exec-ms` file from `summary.json`
- `max-exec`
  - aggregation of the `rules-engine-stats.calculations.max-exec-ms` file from `summary.json`

## xx
### Summary.json File

The elements in the `summary.json` file are organized by the name of the test that
they are executing.  Regardless of whether it is a single test or a group of tests, that first
element is always the name of that test.  After that, there is some divergence
on how the information is stored to more efficiently aggregate the information from a
group of tests.

#### JSON Blob Naming

In most cases, each entry in the specified `suite-*.txt` file will
have a unique name.  When the information for that entry is placed in the `summary.json`
file, the name associated with the blob is the name of the test that was executed.
Therefore, if the specified `suite-*.txt` file is:

```text
smoke
smoke-time-only
```

it is expected that the `summary.json` file will contain one summary named `smoke`
that contains the information for the first test and another summary named `smoke-time-only`
that contains the information for the second test.

To handle the rare cases where there are duplicates, the names of the blobs are
adjusted to avoid collisions.  Depending on the type of blob being added, one or more
underscore characters (`_`) are appended to the test name, followed by an index number
to ensure that the name is unique.  To ensure that there is a direct correlation between
a line in the `suite-*.txt` file and a specified blob, the `source` blob inside of the
test blob identifies the exact source file and line number that generated the blob.

#### Single Test JSON Blob

For a single test, for example the `smoke` test, the output will look something
like the following example.  Note that the information in the `slices` blob has been
omitted as most of that information is aggregated in other places within the
overall JSON blob.

```json
"smoke": {
  "source": {
      "file_name": "/../tests/suite-something.txt",
      "line_number": 1
  },
  "configuration": {
      "thread_pool_count": 1,
      "stats_log_interval": 2,
      "log_individual_rule_stats": true,
      "rule_retry_count": 3
  },
  "iterations": 1023,
  "return-code": 0,
  "duration-sec": 11.230913584,
  "stop-pause-sec": 6.000195129,
  "wait-pause-sec": 4.435873457,
  "print-duration-sec": 0.326038276,
  "start-transaction-sec": 0.046970413,
  "inside-transaction-sec": 0.145943434,
  "end-transaction-sec": 0.07774411,
  "update-row-sec": 0.016623276,
  "average-start-transaction-microsec": 32.230933529,
  "average-inside-transaction-microsec": 120.894697947,
  "average-end-transaction-microsec": 64.898587488,
  "average-update-row-microsec": 16.249536657,
  "average-wait-microsec": 515.813130987,
  "average-print-microsec": 168.943209189,
  "test-duration-sec": 0.4688067220000003,
  "iteration-duration-sec": 0.0004582665904203326,
  "measured-duration-sec": 0.2902219,
  "explicit_wait_in_sec" : 0.0050000,
  "iteration-measured-duration-sec": 0.000283696871945259,
  "rules-engine-stats": {
      "slices": [
        ...
      ],
      "totals": {
          "scheduled": 3222,
          "invoked": 3222,
          "pending": 0,
          "abandoned": 0,
          "retry": 0,
          "exception": 0,
          "individual": {
            ...
          }
      },
      "calculations": {
          "avg-latency-ms": 0.17920235878336438,
          "max-latency-ms": 1.0,
          "avg-exec-ms": 0.01,
          "max-exec-ms": 0.17,
          "individual": {
            ...
          }
      }
  }
}
```

A summary of the various fields and blobs are as follows:

- `source`
  - the source suite file and line number within that file that generated this blob
- `configuration`
  - a summary of the relevant fields in the `mink.conf` file
- `iteration`
  - harvested from `output.delay`
  - number of steps that were enacted as part of the test
- `return-code`
  - return code from `return_code.json`
  - generated from return code of `test.sh`
  - see [this section](#test-return-codes) for specific decodings
- `duration-sec`
  - harvested from `duration.json`
  - amount of time that it took to execute `run.sh`
- `stop-pause-sec`
  - harvested from `output.delay`
  - amount of time that the test paused at the end of its execution
  - allows time for any information in `gaia_stats.log` to be written
- `wait-pause-sec`
  - harvested from `output.delay`
  - sum of total time used by waits performed after the `step` commands
- `explicit_wait_in_sec`
  - harvested from `output.delay`
  - sum of total time used by the `w`ait command from the `commands.txt` file
- `print-duration-sec`
  - harvested from `output.delay`
  - time spent in the `step_and_emit_state` method just to print the current state
- `test-duration-sec`
  - `duration-sec` minus `stop-pause-sec` minus `wait-pause-sec` minus `print-duration-sec`
  - approximation of the run time of the test
- `iteration-duration-sec`
  - `test-duration-sec` divided by `iteration`
  - approximation of the run time of a single step within the test
  - this field will not appear if no steps were reported in the `iteration` field
- `start-transaction-sec`
  - harvested from `output.delay`
  - amount of time spent in each step starting the transaction for the step
- `inside-transaction-sec`
  - harvested from `output.delay`
  - amount of time spent performing the workload for the step
- `end-transaction-sec`
  - harvested from `output.delay`
  - amount of time spent in each step committing the transaction for the step
- `update-row-sec`
  - harvested from `output.delay`
  - amount of time spent in each step updating database rows
- `average-start-transaction-microsec`
  - calculated by dividing the `start-transaction-sec` metric by `iteration`
- `average-inside-transaction-microsec`
  - calculated by dividing the `iteration-duration-sec` metric by `iteration`
- `average-end-transaction-microsec`
  - calculated by dividing the `end-transaction-sec` metric by `iteration`
- `update-row-sec`
  - calculated by dividing the `end-transaction-sec` metric by `iteration`
- `average-wait-microsec`
  - calculated by dividing the `wait-pause-sec` metric by `iteration`
- `average-print-microsec`
  - calculated by dividing the `print-duration-sec` metric by `iteration`
- `measured-duration-sec`
  - optional field triggered by use of the [Toggling Measurements On and Off](#toggling-measurements-on-and-off) command
  - duration that occurred between the measurement being toggled on and off
- `iteration-measured-duration-sec`
  - optional field triggered by use of the [Toggling Measurements On and Off](#toggling-measurements-on-and-off) command
  - approximation of the time of a single step within the measurement
  - this field will not appear if no steps were reported in the `iteration` field
- `rules-engine-stats.slices`
  - raw slices harvested from `gaia_stats.log`
- `totals.*`
  - aggregated values over all `rules-engine-stats.slices`
- `calculations.*`
  - weighted average/maximum values over all `rules-engine-stats.slices`

  - aggregation of the `average-start-transaction-microsec` file from `summary.json`
- `inside`
  - aggregation of the `average-inside-transaction-microsec` file from `summary.json`
- `commit`
  - aggregation of the `average-end-transaction-microsec` file from `summary.json`
- `update-row`
  - aggregation of the `average-update-row-microsec` file from `summary.json`
- `wait`
  - aggregation of the `average-wait-microsec` file from `summary.json`
- `print`
  - aggregation of the `average-print-microsec` file from `summary.json`


## Modifications To The Incubator Example

As the Incubator example is a common example that everyone has experience with,
it made sense to use it as a base for the first integration test.  However, to
be used as the base application for this integration test suite, some small
modifications were required.  While the codebases are still closely related,
sharing common code, this application has been renamed to `Mink` or "Modified
INKubator" to remind test framework users of the difference.

### Command Line

#### Debug Mode

To provide for a solid way to execute tests, a new `debug` command mode was
added.  When specified, the usual `begin simulation` and `end simulation`
commands from the main menu are not available.  Instead, they are replaced
with the `step simulation` command.  Replacing the simulation's proceeding
on a 1 second timer, the `s`/`step simulation` code forces the user to take each
step individually.  In addition, the `z`/`step simulation and emit state` is a
variation of the `step simulation` command that emits a json blob with the
state of the database after the step has completed.

#### Showj Mode

The `showj` command mode is like the `show` command except that it
outputs the state of the database as a JSON blob and then exits.  While the
display is not as compact as the `show` command, the `showj` command's JSON
blob is more structured.

Whether you use the `show` command or the `showj` command boils
down to personal preference.

### Main Menu Commands

To facilitate the `debug` command mode, a collection of new main menu commands
was added.

As part of the testing framework, the usual use of the debug command mode is
for each test to provide for a `commands.txt` file that contains one line for each command
to be executed.  This file is the redirected into the `run.sh` script as part
of the test itself.  Each command in that file is simply the same command that
would be entered if the user were interacting with the application directly.

#### Toggling Measurements On and Off

The `o`/`toggle measurement on and off` command toggles the capturing of
the duration measurement on and off.  While the framework currently only supports
the capturing of a single measurement, that single measurement can be used to
great effect.  Normally, the most accurate scope that is recorded and reported
on is the `test-duration-sec` field of the JSON blobs (see above).  This is a
field calculated by taking the `duration-sec` field and subtracting the
`stop-pause-sec` field, the `wait-pause-sec` field, and the `print-duration-sec`
field.

As those are all high-level measurements, taken over the entire
debug script, they might not provide the required accuracy.  That is where
these commands can be used to provide that accuracy.  By toggling the measurement
on before the interested area in the script and off when it has completed, the
measurement is focused only on that one area of the script.  Once toggled off,
that duration will be reported in the `measured-duration-sec` field of the
`summary.json` file, along with its `iteration-measured-duration-sec` measurement.

#### Step Commands

The `s`/`step simulation` command takes most of the code in the `simulation`
method's while loop and moves it to the `simulation_step` method.  To further
support the idea of an increasing progression of time, the wrapper method
`step` increases the timestamp `g_timestamp` and then calls the `simulation_step`
method.

The `z`/`step simulation and emit state` command is like the `s`/`step simulation`
command, but some extra packaging around the calling of the `step` method.
Calling the `step_and_emit_state` method, that method then calls the `step` method
for the `s`/`step simulation` command, but follows it up with code to wait for
the engine to finish processing the changes, following that up by dumping out a
JSON representation of the database.

The introduction of the `s` command was primarily geared around being able to
proceed with the simulation at a much quicker pace than once a second.  Once
that was accomplished, the `z` command was added to get a predictable record
of the changes that occurred at each step of the simulation.

For example:

```
s
```

will take a single step.  The same example, but with a `z`, will take a single
step, but wait for the step's side effects to subside and print the current
state of the database as a JSON blob.

##### Multiple Step Command

Instead of asking a test developer to insert many step commands in
a `commands.txt` file, a shortcut is available for use.  If a positive integer
is specified directly after the `s` or `z` command, it will be interpreted as
a request to repeat that command the specified number of times.  For example:

```
z1000
```

is asking that the `z` command be executed 1000 times.

##### Future

A logical combination of the two above commands is to provide a step command
that waits for the processing of any changes before starting the next command.

#### Wait Command

Added initially to allow for some cooldown after any initialization and before any
block of heavy testing, the `w` command allows for a number to be specified
after the `w`.  This is the number of milliseconds that the application will
wait before returning for the next command.

For example:

```
w10
```

will wait for 10 milliseconds before proceeding.

As mentioned in the opening sentence, this command was initially added to
allow for some cooldown to occur between any initialization and the actual
testing started.  The intention here is to provide a buffer between the two
sets of actions, hopefully isolating them from each other.

#### Comment Command

As the normal test usage of the `debug` command mode and `tests.sh` is to
redirect the `commands.txt` file for the test into the `run.sh` script,
it was advantageous to be able to include comments.  For example, the
actual `commands.txt` file for the `smoke` test looks like this:

```
# Turn chicken portion on.
m
c
on
m
# Turn puppy portion on.
m
p
on
m
# Wait 5 milliseconds to allow the turn on effects to finish.
w5
# Take 1023 steps.
z1023
```

